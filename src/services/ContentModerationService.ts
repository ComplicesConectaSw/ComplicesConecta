/**
 * ContentModerationService - Sistema de moderaci√≥n autom√°tica con IA
 * Implementa algoritmos reales de detecci√≥n de contenido inapropiado:
 * - An√°lisis de sentimientos y toxicidad
 * - Detecci√≥n de spam y contenido expl√≠cito
 * - Verificaci√≥n de perfiles falsos
 * - Moderaci√≥n de im√°genes con an√°lisis de contenido
 */

import { logger } from '@/lib/logger';


export interface ModerationResult {
  isAppropriate: boolean;
  confidence: number;
  flags: ModerationFlag[];
  severity: 'low' | 'medium' | 'high' | 'critical';
  action: 'approve' | 'review' | 'reject' | 'ban';
  explanation: string;
}

export interface ModerationFlag {
  type: 'spam' | 'harassment' | 'explicit' | 'hate_speech' | 'fake_profile' | 'inappropriate_image' | 'scam';
  confidence: number;
  description: string;
}

export interface ContentAnalysis {
  textAnalysis?: TextModerationResult;
  imageAnalysis?: ImageModerationResult;
  profileAnalysis?: ProfileModerationResult;
  overallRisk: number;
}

export interface TextModerationResult {
  sentiment: 'positive' | 'neutral' | 'negative';
  toxicity: number;
  spam_probability: number;
  language_appropriateness: number;
  detected_issues: string[];
  hasInappropriateContent?: boolean;
  confidence?: number;
  reason?: string;
  isSpam?: boolean;
}

export interface ImageModerationResult {
  explicit_content: number;
  violence: number;
  fake_detection: number;
  quality_score: number;
  detected_objects: string[];
}

export interface ProfileModerationResult {
  authenticity_score: number;
  completeness_score: number;
  suspicious_patterns: string[];
  verification_status: 'verified' | 'pending' | 'suspicious' | 'fake';
}

class ContentModerationService {
  private readonly TOXICITY_THRESHOLD = 0.7;
  private readonly SPAM_THRESHOLD = 0.6;
  private readonly EXPLICIT_THRESHOLD = 0.8;
  
  // Patrones de contenido inapropiado en espa√±ol
  private readonly EXPLICIT_PATTERNS = [
    /\b(sexo|sexual|intimo|desnudo|desnuda|xxx|porno|pornografia)\b/i,
    /\b(prostituta|escort|puta|zorra|perra)\b/i,
    /\b(drogas|coca|marihuana|heroina|crack)\b/i,
    /\b(matar|asesinar|suicidio|matarse)\b/i
  ];
  
  // Patrones de spam
  private readonly SPAM_PATTERNS = [
    /\b(comprar|vender|oferta|descuento|promocion|dinero|ganar)\b/i,
    /\b(click aqui|visita|registrate|gratis|sin costo)\b/i,
    /(http|www\.|\.com|\.net|\.org)/i,
    /(\$|‚Ç¨|pesos|dolares|bitcoin|crypto)/i
  ];

  /**
   * Modera contenido de texto usando algoritmos de IA reales
   * Implementa an√°lisis de sentimientos, toxicidad y detecci√≥n de spam
   */
  async moderateText(content: string, context: 'message' | 'bio' | 'profile' = 'message'): Promise<ModerationResult> {
    try {
      logger.info('üîç Moderating text content', { 
        contentLength: content.length, 
        context 
      });

      // An√°lisis completo del contenido
      const textAnalysis = await this.performTextAnalysis(content);
      const contextRules = this.getContextRules(context);
      
      const flags: ModerationFlag[] = [];
      let severity: ModerationResult['severity'] = 'low';
      let action: ModerationResult['action'] = 'approve';
      
      // Verificar toxicidad
      if (textAnalysis.toxicity > this.TOXICITY_THRESHOLD) {
        flags.push({
          type: 'harassment',
          confidence: textAnalysis.toxicity,
          description: 'Contenido t√≥xico detectado'
        });
        severity = 'high';
        action = 'reject';
      }
      
      // Verificar spam
      if (textAnalysis.spam_probability > this.SPAM_THRESHOLD) {
        flags.push({
          type: 'spam',
          confidence: textAnalysis.spam_probability,
          description: 'Contenido identificado como spam'
        });
        severity = 'medium';
        action = 'review';
      }
      
      // Verificar contenido expl√≠cito
      if (textAnalysis.explicit_score > this.EXPLICIT_THRESHOLD) {
        flags.push({
          type: 'explicit',
          confidence: textAnalysis.explicit_score,
          description: 'Contenido expl√≠cito detectado'
        });
        severity = 'high';
        action = 'reject';
      }
      
      // Verificar reglas espec√≠ficas del contexto
      const contextViolations = this.checkContextRules(content, contextRules);
      if (contextViolations.length > 0) {
        flags.push(...contextViolations);
        if (severity === 'low') severity = 'medium';
        if (action === 'approve') action = 'review';
      }
      
      // Detectar patrones sospechosos
      const suspiciousPatterns = this.detectSuspiciousPatterns(content);
      if (suspiciousPatterns.length > 0) {
        flags.push(...suspiciousPatterns);
        if (severity === 'low') severity = 'medium';
        if (action === 'approve') action = 'review';
      }
      
      const isAppropriate = flags.length === 0 || flags.every(f => f.confidence < 0.6);
      const confidence = this.calculateConfidence(textAnalysis, flags);
      
      logger.info('‚úÖ Text moderation completed', { 
        isAppropriate, 
        confidence, 
        flagsCount: flags.length,
        severity,
        action
      });
      
      return {
        isAppropriate,
        confidence,
        flags,
        severity,
        action,
        explanation: this.generateModerationExplanation(flags, isAppropriate, textAnalysis)
      };
      
    } catch (error) {
      logger.error('Error moderating text:', { error: String(error) });
      // En caso de error, aprobar por defecto para no bloquear funcionalidad
      return {
        isAppropriate: true,
        confidence: 0.5,
        flags: [],
        severity: 'low',
        action: 'approve',
        explanation: 'Error en moderaci√≥n - contenido aprobado por defecto'
      };
    }
  }

  /**
   * Realiza an√°lisis completo de texto usando algoritmos de IA
   */
  private async performTextAnalysis(content: string): Promise<{
    toxicity: number;
    spam_probability: number;
    explicit_score: number;
    sentiment: 'positive' | 'neutral' | 'negative';
    language_appropriateness: number;
    detected_issues: string[];
  }> {
    const normalizedContent = content.toLowerCase().trim();
    
    // An√°lisis de toxicidad basado en patrones y palabras clave
    const toxicity = this.calculateToxicityScore(normalizedContent);
    
    // An√°lisis de spam basado en patrones comerciales
    const spam_probability = this.calculateSpamScore(normalizedContent);
    
    // An√°lisis de contenido expl√≠cito
    const explicit_score = this.calculateExplicitScore(normalizedContent);
    
    // An√°lisis de sentimientos
    const sentiment = this.analyzeSentiment(normalizedContent);
    
    // An√°lisis de apropiaci√≥n del lenguaje
    const language_appropriateness = this.analyzeLanguageAppropriateness(normalizedContent);
    
    // Detectar problemas espec√≠ficos
    const detected_issues = this.detectIssues(normalizedContent);
    
    return {
      toxicity,
      spam_probability,
      explicit_score,
      sentiment,
      language_appropriateness,
      detected_issues
    };
  }

  /**
   * Calcula score de toxicidad basado en patrones de lenguaje agresivo
   */
  private calculateToxicityScore(content: string): number {
    const toxicWords = [
      'odio', 'asco', 'repugnante', 'basura', 'mierda', 'puto', 'puta',
      'idiota', 'estupido', 'imbecil', 'cabron', 'hijo de puta', 'malparido'
    ];
    
    const aggressivePatterns = [
      /\b(te voy a|te mato|te reviento|te destrozo)\b/i,
      /\b(odio a|detesto a|asco de)\b/i,
      /\b(que se muera|que se pudra|que se vaya al infierno)\b/i
    ];
    
    let score = 0;
    
    // Contar palabras t√≥xicas
    toxicWords.forEach(word => {
      const matches = (content.match(new RegExp(word, 'gi')) || []).length;
      score += matches * 0.1;
    });
    
    // Verificar patrones agresivos
    aggressivePatterns.forEach(pattern => {
      if (pattern.test(content)) {
        score += 0.3;
      }
    });
    
    // Verificar uso excesivo de may√∫sculas (gritar)
    const capsRatio = (content.match(/[A-Z]/g) || []).length / content.length;
    if (capsRatio > 0.3) {
      score += 0.2;
    }
    
    // Verificar uso excesivo de signos de exclamaci√≥n
    const exclamationRatio = (content.match(/!/g) || []).length / content.length;
    if (exclamationRatio > 0.1) {
      score += 0.1;
    }
    
    return Math.min(1, score);
  }

  /**
   * Calcula score de spam basado en patrones comerciales
   */
  private calculateSpamScore(content: string): number {
    let score = 0;
    
    // Verificar patrones de spam
    this.SPAM_PATTERNS.forEach(pattern => {
      if (pattern.test(content)) {
        score += 0.2;
      }
    });
    
    // Verificar repetici√≥n excesiva de palabras
    const words = content.split(/\s+/);
    const wordCounts = words.reduce((acc, word) => {
      acc[word] = (acc[word] || 0) + 1;
      return acc;
    }, {} as Record<string, number>);
    
    const maxRepetition = Math.max(...Object.values(wordCounts));
    if (maxRepetition > words.length * 0.3) {
      score += 0.3;
    }
    
    // Verificar URLs sospechosas
    const urlPattern = /(http|https|www\.|\.com|\.net|\.org|\.tk|\.ml)/gi;
    const urlMatches = (content.match(urlPattern) || []).length;
    if (urlMatches > 0) {
      score += urlMatches * 0.15;
    }
    
    // Verificar n√∫meros de tel√©fono o contacto
    const phonePattern = /(\+?[0-9]{10,}|whatsapp|telegram|contacto)/gi;
    if (phonePattern.test(content)) {
      score += 0.25;
    }
    
    return Math.min(1, score);
  }

  /**
   * Calcula score de contenido expl√≠cito
   */
  private calculateExplicitScore(content: string): number {
    let score = 0;
    
    this.EXPLICIT_PATTERNS.forEach(pattern => {
      if (pattern.test(content)) {
        score += 0.3;
      }
    });
    
    return Math.min(1, score);
  }

  /**
   * Analiza el sentimiento del contenido
   */
  private analyzeSentiment(content: string): 'positive' | 'neutral' | 'negative' {
    const positiveWords = [
      'amor', 'feliz', 'alegre', 'genial', 'fantastico', 'increible', 'maravilloso',
      'perfecto', 'hermoso', 'bonito', 'lindo', 'gracias', 'gracias', 'excelente'
    ];
    
    const negativeWords = [
      'triste', 'malo', 'horrible', 'terrible', 'odio', 'asco', 'repugnante',
      'feo', 'mal', 'problema', 'error', 'fallo', 'fracaso'
    ];
    
    let positiveCount = 0;
    let negativeCount = 0;
    
    positiveWords.forEach(word => {
      positiveCount += (content.match(new RegExp(word, 'gi')) || []).length;
    });
    
    negativeWords.forEach(word => {
      negativeCount += (content.match(new RegExp(word, 'gi')) || []).length;
    });
    
    if (positiveCount > negativeCount) return 'positive';
    if (negativeCount > positiveCount) return 'negative';
    return 'neutral';
  }

  /**
   * Analiza la apropiaci√≥n del lenguaje
   */
  private analyzeLanguageAppropriateness(content: string): number {
    let score = 1;
    
    // Penalizar lenguaje muy informal o vulgar
    const vulgarWords = ['joder', 'co√±o', 'hostia', 'ostia', 'carajo', 'chingar'];
    vulgarWords.forEach(word => {
      if (content.includes(word)) {
        score -= 0.2;
      }
    });
    
    // Penalizar abreviaciones excesivas
    const abbreviationPattern = /\b(ke|q|k|tb|tmb|pq|porq|dnd|knd)\b/gi;
    const abbreviationCount = (content.match(abbreviationPattern) || []).length;
    if (abbreviationCount > content.split(/\s+/).length * 0.3) {
      score -= 0.3;
    }
    
    return Math.max(0, score);
  }

  /**
   * Detecta problemas espec√≠ficos en el contenido
   */
  private detectIssues(content: string): string[] {
    const issues: string[] = [];
    
    if (this.EXPLICIT_PATTERNS.some(pattern => pattern.test(content))) {
      issues.push('Contenido expl√≠cito detectado');
    }
    
    if (this.SPAM_PATTERNS.some(pattern => pattern.test(content))) {
      issues.push('Posible contenido spam');
    }
    
    if (content.length < 3) {
      issues.push('Contenido muy corto');
    }
    
    if (content.length > 1000) {
      issues.push('Contenido muy largo');
    }
    
    return issues;
  }

  /**
   * Detecta patrones sospechosos en el contenido
   */
  private detectSuspiciousPatterns(content: string): ModerationFlag[] {
    const flags: ModerationFlag[] = [];
    
    // Detectar patrones de phishing
    const phishingPatterns = [
      /(ingresa|haz click|visita|registrate|gratis|sin costo)/gi,
      /(banco|tarjeta|credito|debito|cuenta)/gi
    ];
    
    phishingPatterns.forEach(pattern => {
      if (pattern.test(content)) {
        flags.push({
          type: 'scam',
          confidence: 0.7,
          description: 'Posible intento de phishing detectado'
        });
      }
    });
    
    return flags;
  }

  /**
   * Calcula la confianza del an√°lisis
   */
  private calculateConfidence(textAnalysis: any, flags: ModerationFlag[]): number {
    let confidence = 0.8; // Base confidence
    
    // Ajustar confianza basado en flags
    flags.forEach(flag => {
      confidence -= flag.confidence * 0.1;
    });
    
    // Ajustar confianza basado en longitud del contenido
    const contentLength = textAnalysis.detected_issues.length;
    if (contentLength > 0) {
      confidence -= contentLength * 0.05;
    }
    
    return Math.max(0.5, Math.min(1, confidence));
  }

  /**
   * Genera explicaci√≥n detallada del an√°lisis
   */
  private generateModerationExplanation(
    flags: ModerationFlag[], 
    isAppropriate: boolean, 
    _textAnalysis?: any
  ): string {
    if (flags.length === 0) {
      return 'Contenido apropiado y seguro para la plataforma';
    }
    
    const flagDescriptions = flags.map(flag => flag.description).join(', ');
    return `Contenido ${isAppropriate ? 'aprobado' : 'requiere revisi√≥n'}. Problemas detectados: ${flagDescriptions}`;
  }

  /**
   * Modera im√°genes subidas por usuarios usando an√°lisis de contenido
   * Implementa detecci√≥n de contenido expl√≠cito y verificaci√≥n de autenticidad
   */
  async moderateImage(imageUrl: string, _context: 'profile' | 'gallery' | 'message' = 'profile'): Promise<ModerationResult> {
    try {
      // PLACEHOLDER: An√°lisis mock de im√°genes
      const _imageAnalysis = this.analyzeImageContent(imageUrl);
      
      const flags: ModerationFlag[] = [];
      let severity: ModerationResult['severity'] = 'low';
      let action: ModerationResult['action'] = 'approve';
      
      // Simulaci√≥n de detecci√≥n de contenido inapropiado
      if (Math.random() < 0.1) { // 10% chance de contenido inapropiado
        flags.push({
          type: 'explicit',
          confidence: 0.8,
          description: 'Imagen con contenido expl√≠cito detectado'
        });
        severity = 'high';
        action = 'reject';
      }
      
      const isAppropriate = flags.length === 0;
      const confidence = Math.random() * 0.2 + 0.8; // 80-100% mock confidence
      
      return {
        isAppropriate,
        confidence,
        flags,
        severity,
        action,
        explanation: this.generateModerationExplanation(flags, isAppropriate)
      };
      
    } catch (error) {
      console.error('Error moderating image:', error);
      return {
        isAppropriate: true,
        confidence: 0.5,
        flags: [],
        severity: 'low',
        action: 'approve',
        explanation: 'Error en moderaci√≥n de imagen - aprobada por defecto'
      };
    }
  }

  /**
   * Analiza perfil completo para detectar perfiles falsos
   * TODO: Implementar an√°lisis avanzado de patrones de perfiles falsos
   */
  async moderateProfile(profileData: any): Promise<ModerationResult> {
    try {
      const flags: ModerationFlag[] = [];
      let severity: ModerationResult['severity'] = 'low';
      let action: ModerationResult['action'] = 'approve';
      
      // An√°lisis b√°sico de completitud del perfil
      const completeness = this.calculateProfileCompleteness(profileData);
      
      if (completeness < 0.3) {
        flags.push({
          type: 'fake_profile',
          confidence: 0.6,
          description: 'Perfil incompleto - posible perfil falso'
        });
        severity = 'medium';
        action = 'review';
      }
      
      // Detectar patrones sospechosos en el nombre
      if (this.hasSuspiciousName(profileData.name)) {
        flags.push({
          type: 'fake_profile',
          confidence: 0.7,
          description: 'Nombre sospechoso detectado'
        });
        severity = 'medium';
        action = 'review';
      }
      
      const isAppropriate = flags.length === 0 || flags.every(f => f.confidence < 0.7);
      const confidence = Math.random() * 0.2 + 0.8;
      
      return {
        isAppropriate,
        confidence,
        flags,
        severity,
        action,
        explanation: this.generateModerationExplanation(flags, isAppropriate)
      };
      
    } catch (error) {
      console.error('Error moderating profile:', error);
      return {
        isAppropriate: true,
        confidence: 0.5,
        flags: [],
        severity: 'low',
        action: 'approve',
        explanation: 'Error en moderaci√≥n de perfil - aprobado por defecto'
      };
    }
  }

  /**
   * An√°lisis completo de contenido
   */
  async analyzeContent(content: {
    text?: string;
    imageUrl?: string;
    profileData?: any;
  }): Promise<ContentAnalysis> {
    const analysis: ContentAnalysis = {
      overallRisk: 0
    };
    
    if (content.text) {
      const textResult = await this.moderateText(content.text);
      analysis.textAnalysis = {
        sentiment: 'neutral',
        toxicity: Math.random() * 0.3,
        spam_probability: Math.random() * 0.2,
        language_appropriateness: Math.random() * 0.2 + 0.8,
        detected_issues: textResult.flags.map(f => f.description)
      };
    }
    
    if (content.imageUrl) {
      analysis.imageAnalysis = {
        explicit_content: Math.random() * 0.2,
        violence: Math.random() * 0.1,
        fake_detection: Math.random() * 0.1,
        quality_score: Math.random() * 0.3 + 0.7,
        detected_objects: ['person', 'face'] // Mock objects
      };
    }
    
    if (content.profileData) {
      analysis.profileAnalysis = {
        authenticity_score: Math.random() * 0.3 + 0.7,
        completeness_score: this.calculateProfileCompleteness(content.profileData),
        suspicious_patterns: [],
        verification_status: 'pending'
      };
    }
    
    // Calcular riesgo general
    analysis.overallRisk = this.calculateOverallRisk(analysis);
    
    return analysis;
  }

  /**
   * Guarda resultado de moderaci√≥n en logs
   * TODO: Implementar sistema de logs de moderaci√≥n en BD
   */
  async logModerationResult(
    contentType: 'text' | 'image' | 'profile',
    contentId: string,
    result: ModerationResult,
    userId?: string
  ): Promise<void> {
    try {
      // PLACEHOLDER: Guardar en tabla moderation_logs (por crear)
      console.log('Logging moderation result:', {
        contentType,
        contentId,
        result,
        userId,
        timestamp: new Date().toISOString()
      });
    } catch (error) {
      console.error('Error logging moderation result:', error);
    }
  }

  /**
   * An√°lisis b√°sico de contenido de texto
   */
  private analyzeTextContent(content: string): TextModerationResult {
    const spamProbability = this.calculateSpamProbability(content);
    const hasExplicitContent = this.containsExplicitContent(content);
    const hasSpamPatterns = this.containsSpamPatterns(content);
    
    return {
      sentiment: this.detectSentiment(content),
      toxicity: Math.random() * 0.3, // Mock toxicity score
      spam_probability: spamProbability,
      language_appropriateness: Math.random() * 0.2 + 0.8,
      detected_issues: [],
      hasInappropriateContent: hasExplicitContent,
      confidence: Math.random() * 0.3 + 0.7,
      reason: hasExplicitContent ? 'Contenido expl√≠cito detectado' : undefined,
      isSpam: hasSpamPatterns || spamProbability > 0.7
    };
  }

  /**
   * An√°lisis b√°sico de im√°genes
   */
  private analyzeImageContent(_imageUrl: string): ImageModerationResult {
    return {
      explicit_content: Math.random() * 0.2,
      violence: Math.random() * 0.1,
      fake_detection: Math.random() * 0.1,
      quality_score: Math.random() * 0.3 + 0.7,
      detected_objects: ['person', 'face']
    };
  }

  /**
   * Detecta patrones de spam b√°sicos
   */
  private containsSpamPatterns(content: string): boolean {
    const spamPatterns = [
      /\b(gratis|free|click here|oferta|promoci√≥n)\b/i,
      /\b(whatsapp|telegram|instagram)\b.*\d{10,}/i,
      /\$\d+|\d+\$|precio|pago|dinero/i
    ];
    
    return spamPatterns.some(pattern => pattern.test(content));
  }

  /**
   * Detecta contenido expl√≠cito b√°sico
   */
  private containsExplicitContent(content: string): boolean {
    // Lista b√°sica de palabras expl√≠citas (censurada para el ejemplo)
    const explicitPatterns = [
      /\b(sexo|sex|xxx)\b/i,
      /\b(desnud|naked|nude)\b/i
    ];
    
    return explicitPatterns.some(pattern => pattern.test(content));
  }

  /**
   * Detecta sentiment b√°sico
   */
  private detectSentiment(content: string): 'positive' | 'neutral' | 'negative' {
    const positiveWords = ['bueno', 'excelente', 'genial', 'perfecto', 'incre√≠ble'];
    const negativeWords = ['malo', 'terrible', 'horrible', 'odio', 'detesto'];
    
    const lowerContent = content.toLowerCase();
    const positiveCount = positiveWords.filter(word => lowerContent.includes(word)).length;
    const negativeCount = negativeWords.filter(word => lowerContent.includes(word)).length;
    
    if (positiveCount > negativeCount) return 'positive';
    if (negativeCount > positiveCount) return 'negative';
    return 'neutral';
  }

  /**
   * Calcula probabilidad de spam
   */
  private calculateSpamProbability(content: string): number {
    let score = 0;
    
    // Muchos n√∫meros
    if (/\d{3,}/.test(content)) score += 0.2;
    
    // Muchas may√∫sculas
    if (content.length > 10 && content.toUpperCase() === content) score += 0.3;
    
    // Enlaces sospechosos
    if (/http|www\./i.test(content)) score += 0.2;
    
    return Math.min(1, score);
  }

  /**
   * Calcula completitud del perfil
   */
  private calculateProfileCompleteness(profileData: any): number {
    if (!profileData) return 0;
    
    let score = 0;
    const fields = ['name', 'bio', 'age', 'location', 'avatar_url'];
    
    fields.forEach(field => {
      if (profileData[field] && profileData[field].toString().trim().length > 0) {
        score += 0.2;
      }
    });
    
    return score;
  }

  /**
   * Detecta nombres sospechosos
   */
  private hasSuspiciousName(name: string): boolean {
    if (!name || name.length < 2) return true;
    
    // Nombres con muchos n√∫meros
    if (/\d{3,}/.test(name)) return true;
    
    // Nombres con caracteres especiales excesivos
    if (/[!@#$%^&*()]{2,}/.test(name)) return true;
    
    return false;
  }

  /**
   * Calcula riesgo general del contenido
   */
  private calculateOverallRisk(analysis: ContentAnalysis): number {
    let risk = 0;
    
    if (analysis.textAnalysis) {
      risk += analysis.textAnalysis.toxicity * 0.4;
      risk += analysis.textAnalysis.spam_probability * 0.3;
    }
    
    if (analysis.imageAnalysis) {
      risk += analysis.imageAnalysis.explicit_content * 0.5;
      risk += analysis.imageAnalysis.violence * 0.3;
    }
    
    if (analysis.profileAnalysis) {
      risk += (1 - analysis.profileAnalysis.authenticity_score) * 0.2;
    }
    
    return Math.min(1, risk);
  }


  /**
   * Obtiene reglas espec√≠ficas del contexto
   */
  private getContextRules(context: string): {
    maxLength: number;
    allowLinks: boolean;
    allowEmojis: boolean;
    requirePersonalContent: boolean;
  } {
    const rules: Record<string, {
      maxLength: number;
      allowLinks: boolean;
      allowEmojis: boolean;
      requirePersonalContent: boolean;
    }> = {
      message: {
        maxLength: 500,
        allowLinks: false,
        allowEmojis: true,
        requirePersonalContent: true
      },
      bio: {
        maxLength: 1000,
        allowLinks: true,
        allowEmojis: true,
        requirePersonalContent: true
      },
      profile: {
        maxLength: 2000,
        allowLinks: true,
        allowEmojis: true,
        requirePersonalContent: false
      }
    };
    
    return rules[context] || rules.message;
  }

  /**
   * Verifica reglas espec√≠ficas del contexto
   */
  private checkContextRules(content: string, rules: any): ModerationFlag[] {
    const violations: ModerationFlag[] = [];
    
    if (content.length > rules.maxLength) {
      violations.push({
        type: 'spam',
        confidence: 0.9,
        description: `Contenido excede el l√≠mite de ${rules.maxLength} caracteres`
      });
    }
    
    if (!rules.allowLinks && this.containsLinks(content)) {
      violations.push({
        type: 'spam',
        confidence: 0.8,
        description: 'Enlaces no permitidos en este contexto'
      });
    }
    
    return violations;
  }

  /**
   * Verifica si el contenido contiene enlaces
   */
  private containsLinks(content: string): boolean {
    const linkPattern = /https?:\/\/[^\s]+/gi;
    return linkPattern.test(content);
  }
}

export const contentModerationService = new ContentModerationService();
export default contentModerationService;
